{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bejoyjon/Data-science/blob/master/HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WTF is HuggingFace?**\n",
        "\n",
        "Hugging Face is like a Github specifically for ML Models.\n",
        "<<describe how it works here>>\n",
        "\n",
        "**Source - https://www.datacamp.com/datalab/w/ddd92206-5362-4269-86de-bdafdd5b97a0/edit**"
      ],
      "metadata": {
        "id": "J08zUJj7QWjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check HuggingFace capabilities for various AI related tasks**"
      ],
      "metadata": {
        "id": "JE1MZFTgSVpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import cell"
      ],
      "metadata": {
        "id": "NPtt47AVebdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "DYfwRL9NSk4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline Options**\n",
        "\n",
        "Using Pipelines, the available tasks are:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " 'audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY'"
      ],
      "metadata": {
        "id": "VgR-xhEAc01x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0wPRWx7SH9la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define all global variables here:\n"
      ],
      "metadata": {
        "id": "bRg7l6lNH9Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TABLE_MODEL_NAME = \"google/tapas-base-finetuned-wtq\""
      ],
      "metadata": {
        "id": "fV6SMIHTH59i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use pipelines (\"add difference between pipelines and direct model usage here\")"
      ],
      "metadata": {
        "id": "BMW2lKkvH32C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#document_pipeline = pipeline(\"document-question-answering\", model=\"\")\n",
        "table_pipeline = pipeline(\"table-question-answering\", model=TABLE_MODEL_NAME)"
      ],
      "metadata": {
        "id": "-QIHwAj4aWTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or alternatively, you can use the model directly"
      ],
      "metadata": {
        "id": "zhnjXSlLGgYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TABLE_MODEL_NAME)\n",
        "model = AutoModelForTableQuestionAnswering.from_pretrained(TABLE_MODEL_NAME)"
      ],
      "metadata": {
        "id": "ETJRvZG7Gnay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Inference providers is also an option, though it leads to billing, and therefore is not a serious choice for learning. e.g.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(\n",
        "  provider = 'auto',\n",
        "  api_key = os.environ['HF_TOKEN']\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KNGLW66zGnuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AutoModel and AutoTokenizer**\n",
        "Using HuggingFace AutoModel and AutoTokenizer - HuggingFace saves models and their associated pre-processors in the HF hub. AutoModel and AutoTokenizer have inbuilt methods to handle all models.\n",
        "\n",
        "The catch here is - AutoModel and AutoTokenizer can only load the base model and not the task-finetuned ones.\n",
        "\n",
        "* AutoTokenizer works for text because the pre-processing done for text is tokenization.\n",
        "\n"
      ],
      "metadata": {
        "id": "HyPZkZn9XC1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import os,torch, huggingface_hub as hf_hub, datasets\n",
        "except Exception as e:\n",
        "  print(\"Libs not installed yet, using !pip install now......\")\n",
        "  !pip install torch huggingface_hub datasets pyarrow transformers\n",
        "  import os,torch, huggingface_hub as hf_hub, datasets, transformers\n",
        "  pass"
      ],
      "metadata": {
        "id": "jKGi10B4_qrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
        "model = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")"
      ],
      "metadata": {
        "id": "DE65m_t2Cgqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the various parameters of the tokenizer and model by \"printing\" the object."
      ],
      "metadata": {
        "id": "87Px-PEKSM73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "PwL5CyXhDWJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we need to use the Model for Sequence Classification tasks? We use the next most generic class called AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "rGj6pqT3U-LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
      ],
      "metadata": {
        "id": "RZgBiX38EExV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An even more specific approach is to use the actual class in the transformer library associated with a particular model."
      ],
      "metadata": {
        "id": "yPpa03yXaMzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
      ],
      "metadata": {
        "id": "bAicvMMoa_0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the above knowledge, we can build an NLP pipeline with the Flan-t5-base model."
      ],
      "metadata": {
        "id": "Z0upsVUYeir0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
      ],
      "metadata": {
        "id": "IGVoC-Ljewqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_language=\"English\"\n",
        "output_language=\"Dutch\"\n",
        "input_question = \"How many kilometers from Washington DC to Miami beach?\"\n",
        "input_text = f\"translate {input_language} to {output_language}: {input_question}\" # you can even create a list of input texts for batch processing"
      ],
      "metadata": {
        "id": "wTfWWtCve7r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(input_text, return_tensors = \"pt\") #returns a dictionary with tensor of type pt (PyTorch)"
      ],
      "metadata": {
        "id": "QiABdiFEheIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<<Explain why we need torch here. Is it an inference machine??>>"
      ],
      "metadata": {
        "id": "AOeAu69ZiX8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): #we needed to mention return_tensors = \"pt\" because we were going to use PyTorch.\n",
        "  outputs = model.generate(**inputs)\n",
        "\n",
        "print(tokenizer.decode(outputs[0])) #outputs is going to be a list of same size as input_text"
      ],
      "metadata": {
        "id": "Gzp4U6RMiesW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "HuggingFace",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}