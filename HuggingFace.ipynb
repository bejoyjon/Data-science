{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bejoyjon/Data-science/blob/master/HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check HuggingFace capabilities for various AI related tasks**"
      ],
      "metadata": {
        "id": "JE1MZFTgSVpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import cell"
      ],
      "metadata": {
        "id": "NPtt47AVebdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "DYfwRL9NSk4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline Options**\n",
        "\n",
        "Using Pipelines, the available tasks are:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " 'audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY'"
      ],
      "metadata": {
        "id": "VgR-xhEAc01x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0wPRWx7SH9la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define all global variables here:\n"
      ],
      "metadata": {
        "id": "bRg7l6lNH9Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TABLE_MODEL_NAME = \"google/tapas-base-finetuned-wtq\""
      ],
      "metadata": {
        "id": "fV6SMIHTH59i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use pipelines (\"add difference between pipelines and direct model usage here\")"
      ],
      "metadata": {
        "id": "BMW2lKkvH32C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#document_pipeline = pipeline(\"document-question-answering\", model=\"\")\n",
        "table_pipeline = pipeline(\"table-question-answering\", model=TABLE_MODEL_NAME)"
      ],
      "metadata": {
        "id": "-QIHwAj4aWTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or alternatively, you can use the model directly"
      ],
      "metadata": {
        "id": "zhnjXSlLGgYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTableQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TABLE_MODEL_NAME)\n",
        "model = AutoModelForTableQuestionAnswering.from_pretrained(TABLE_MODEL_NAME)"
      ],
      "metadata": {
        "id": "ETJRvZG7Gnay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Inference providers is also an option, though it leads to billing, and therefore is not a serious choice for learning. e.g.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(\n",
        "  provider = 'auto',\n",
        "  api_key = os.environ['HF_TOKEN']\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KNGLW66zGnuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import os,torch, huggingface_hub as hf_hub, datasets, transformers\n",
        "except Exception as e:\n",
        "  print(\"Libs not installed yet, using !pip install now......\")\n",
        "  !pip install torch huggingface_hub datasets pyarrow\n",
        "  import os,torch, huggingface_hub as hf_hub, datasets, transformers\n",
        "  pass"
      ],
      "metadata": {
        "id": "jKGi10B4_qrb"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}